# TinyStories Transformer: Small Language Model (20M parameters)
A compact generative Transformer model trained on the TinyStories dataset. This 20-million parameter Small Language Model (SLM) demonstrates basic story generation capabilities while being lightweight enough to run on consumer hardware

## üéØ Project Overview
This project implements a decoder-only Transformer architecture from scratch in PyTorch, specifically designed for text generation. The model is trained on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset - a collection of simple children's stories generated by GPT-3.5/4.

### Model Specifications

| Parameter | Value | Description |
|-----------|-------|-------------|
| **Total Parameters** | 20M | Model size |
| **Vocabulary Size** | 10,000 | Unique tokens |
| **Embedding Dimension** | 512 | Token representation size |
| **Transformer Blocks** | 4 | Number of layers |
| **Attention Heads** | 8 per layer | Multi-head attention |
| **Context Window** | 256 tokens | Maximum sequence length |
| **Feed-Forward Dim** | 1024 | Hidden layer size |
| **Dropout Rate** | 0.2 | Regularization |
| **Activation** | GELU | Activation function |

### Training Details

| Metric | Value | Notes |
|--------|-------|-------|
| **Training Epochs** | 19 | Full training cycles |
| **Batch Size** | 64 | Samples per batch |
| **Learning Rate** | 1e-4 | Adam optimizer |
| **Training Time** | ~16 hours | On NVIDIA RTX 3060ti |
| **Final Training Loss** | 1.06 | Cross-entropy loss |
| **Validation Loss** | 1.01 | On validation data |


# üìù Generation Example
    Prompt: "Once upon a time , there was a boy named Sam who loved his red hat . One day :". 
    Generation result: "Sam went to the park with his red hat . He put the hat on and ran around the park . But when Sam got to the park , he realized that his hat was missing ! He started to cry . Sam \' s mom came over and asked , " What \' s wrong , Sam ?" Sam replied , " My hat is gone ! I can \' t find it anywhere ." Mom said , " Don \' t worry , Sam . We can replace it with a new one . Or maybe we can go to the store and get a new one . It \' ll be as good as new !" Sam smiled . He was so happy that his mom had removed his hat to replace his old one . He went to the store and soon forgot about his hat ."

# üìä Performance Analysis
- ‚úÖ **Grammar & Syntax:** Produces mostly grammatically correct sentences
- ‚úÖ **Style Consistency:** Maintains simple children's story style
- ‚úÖ **Context Following:** Keeps track of characters and basic plot within short sequences
- ‚úÖ **Vocabulary:** Uses appropriate simple language

# üéØ Conclusion & Recommendations
The 20M parameter TinyStories Transformer successfully demonstrates basic language generation capabilities, producing grammatically coherent short stories with consistent style. The model achieves a validation loss of 1.01 after 19 epochs, showing solid convergence on the dataset.

Key takeaway: While this compact model works well for its size, scaling up to 35-50 million parameters is likely to yield significantly better results in terms of:

- Improved logical consistency and reduced "hallucinations"

- Longer coherent narratives (beyond 200 tokens)

- Better handling of complex sentence structures

- More creative and diverse story generation

# ‚öôÔ∏è Configuration Setup
To configure the model, you need to create a configuration file: "config.yaml", based on the template: "config.template.yaml"
